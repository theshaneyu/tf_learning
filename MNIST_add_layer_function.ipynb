{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y_true = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__把加layer寫成一個function__\n",
    "\n",
    "這編寫的add_layer其實只是幫我做了把input資料，乘上一個W矩陣加上bias，也就是一層fully connected層的意思，最後activation function再經過一個softmax，最後輸出就是網路的輸出，已經可以和ground truth label算cross entropy\n",
    "\n",
    "__NOTE: __裡面的 __W__ 和 __bias__ 宣告的方式會影響其初始化的方式，往往對訓練品質會有很大的影響，需要作好的調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    \"\"\"\n",
    "    args1 input: 輸入input的placeholder\n",
    "    args2 in_size: input維度\n",
    "    args3 out_size: output維度\n",
    "    args4 activation_function: 激勵函數，通常是tf.nn下的函式\n",
    "    \"\"\"\n",
    "    W = tf.Variable(tf.zeros([in_size, out_size]))\n",
    "    bias = tf.Variable(tf.zeros([out_size]) + 0.1)\n",
    "    WX_plus_b = tf.matmul(inputs, W) + bias\n",
    "    if activation_function is None:\n",
    "        outputs = WX_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(WX_plus_b)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ML第一步-定義模型__\n",
    "\n",
    "這邊使用寫好的add_layer函式，使用softmax函式，輸入維度維784，輸出維度為10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_1_output = add_layer(X, 784, 512, activation_function=tf.nn.relu)\n",
    "hidden_layer_2_output = add_layer(hidden_layer_1_output, 512, 512, activation_function=tf.nn.relu)\n",
    "output = add_layer(hidden_layer_2_output, 512, 10, activation_function=tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = add_layer(X, 784, 10, activation_function=tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML第二步-定義Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(output), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML第三步-Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "寫一個evaluation function方便之後計算accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return accuracy\n",
    "def evaluation(feed_dict):\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y_true, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return sess.run(accuracy, feed_dict = feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "開始執行computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 輪訓練的loss為 2.30259 , accuracy為 0.14\n",
      "第 50 輪訓練的loss為 2.30282 , accuracy為 0.09\n",
      "第 100 輪訓練的loss為 2.30188 , accuracy為 0.17\n",
      "第 150 輪訓練的loss為 2.30286 , accuracy為 0.1\n",
      "第 200 輪訓練的loss為 2.30212 , accuracy為 0.12\n",
      "第 250 輪訓練的loss為 2.30082 , accuracy為 0.19\n",
      "第 300 輪訓練的loss為 2.30413 , accuracy為 0.07\n",
      "第 350 輪訓練的loss為 2.30065 , accuracy為 0.15\n",
      "第 400 輪訓練的loss為 2.30173 , accuracy為 0.14\n",
      "第 450 輪訓練的loss為 2.30194 , accuracy為 0.12\n",
      "第 500 輪訓練的loss為 2.30157 , accuracy為 0.13\n",
      "第 550 輪訓練的loss為 2.2985 , accuracy為 0.18\n",
      "第 600 輪訓練的loss為 2.29987 , accuracy為 0.14\n",
      "第 650 輪訓練的loss為 2.2992 , accuracy為 0.16\n",
      "第 700 輪訓練的loss為 2.30289 , accuracy為 0.12\n",
      "第 750 輪訓練的loss為 2.2994 , accuracy為 0.14\n",
      "第 800 輪訓練的loss為 2.30596 , accuracy為 0.05\n",
      "第 850 輪訓練的loss為 2.30047 , accuracy為 0.1\n",
      "第 900 輪訓練的loss為 2.30499 , accuracy為 0.08\n",
      "第 950 輪訓練的loss為 2.30488 , accuracy為 0.1\n",
      "第 1000 輪訓練的loss為 2.30314 , accuracy為 0.13\n",
      "第 1050 輪訓練的loss為 2.29949 , accuracy為 0.12\n",
      "第 1100 輪訓練的loss為 2.29903 , accuracy為 0.11\n",
      "第 1150 輪訓練的loss為 2.30089 , accuracy為 0.09\n",
      "第 1200 輪訓練的loss為 2.30615 , accuracy為 0.09\n",
      "第 1250 輪訓練的loss為 2.30402 , accuracy為 0.12\n",
      "第 1300 輪訓練的loss為 2.29709 , accuracy為 0.12\n",
      "第 1350 輪訓練的loss為 2.30157 , accuracy為 0.1\n",
      "第 1400 輪訓練的loss為 2.3094 , accuracy為 0.07\n",
      "第 1450 輪訓練的loss為 2.29844 , accuracy為 0.12\n",
      "第 1500 輪訓練的loss為 2.30648 , accuracy為 0.05\n",
      "第 1550 輪訓練的loss為 2.29909 , accuracy為 0.13\n",
      "第 1600 輪訓練的loss為 2.30036 , accuracy為 0.11\n",
      "第 1650 輪訓練的loss為 2.30813 , accuracy為 0.11\n",
      "第 1700 輪訓練的loss為 2.30176 , accuracy為 0.12\n",
      "第 1750 輪訓練的loss為 2.29989 , accuracy為 0.13\n",
      "第 1800 輪訓練的loss為 2.305 , accuracy為 0.13\n",
      "第 1850 輪訓練的loss為 2.30377 , accuracy為 0.11\n",
      "第 1900 輪訓練的loss為 2.30764 , accuracy為 0.1\n",
      "第 1950 輪訓練的loss為 2.30095 , accuracy為 0.13\n",
      "訓練花費 14.372042655944824 秒\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start = time()\n",
    "\n",
    "for i in range(2000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "#     sess.run(train_step, feed_dict={X: batch_xs, y_true: batch_ys})\n",
    "    \"\"\"\n",
    "    如果執行sess.run的時候，也一起執行cross_entropy這個node\n",
    "    則sess.run可以回傳「當次iteration的loss」，如以下程式碼\n",
    "    \"\"\"\n",
    "    _, current_loss = sess.run([train_step, cross_entropy], feed_dict={X: batch_xs, y_true: batch_ys})\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print('第', i, '輪訓練的loss為', current_loss, ', accuracy為', evaluation({X: batch_xs, y_true: batch_ys}))\n",
    "        \n",
    "end = time()\n",
    "print('訓練花費', (end-start), '秒')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data Accuracy:  0.1135\n",
      "Training Data Accuracy:  0.112345\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y_true, 1)) # 注意這邊的 y 和 y_ 都是placeholder\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Testing Data Accuracy: ', evaluation({X: mnist.test.images, y_true: mnist.test.labels}))\n",
    "print('Training Data Accuracy: ', evaluation({X: mnist.train.images, y_true: mnist.train.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tf_learning",
   "language": "python",
   "name": "venv_tf_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
